{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders\n",
    "from numpy.linalg.linalg import LinAlgError\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations as Cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "def normalization(training_numpy, testing_numpy):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testing_x_scaled = min_max.transform(testing_numpy)\n",
    "    #training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    #testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (x_scaled, testing_x_scaled)\n",
    "\n",
    "def pca_function(rate, data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    for thres_n in xrange(1,len(data)):\n",
    "        if sum(pca.explained_variance_ratio_[:thres_n])>rate:\n",
    "            pca_n = thres_n\n",
    "            break\n",
    "    \n",
    "    pca = PCA(n_components=pca_n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data, pca)\n",
    "\n",
    "def lda_whole_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "def lda_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        print np.unique(original_y)\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "'''\n",
    "def pca_function(n, data):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data.flatten(), pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    lda = LDA()\n",
    "    lda_data = lda.fit_transform(X, y)\n",
    "    return (lda_data.flatten(), lda)\n",
    "'''\n",
    "def replace_value(training_normal_numpy, key, loc):  \n",
    "    training_normal_numpy[loc][key] = 1\n",
    "    \n",
    "def del_zero_column(df_train, df_test):\n",
    "    new_df_test = pd.DataFrame(np.zeros([df_test.shape[0], df_train.shape[1]]),columns=df_train.columns)\n",
    "    new_df_test[list(set(df_train.columns) & set(df_test.columns))] = df_test[list(set(df_train.columns) & set(df_test.columns))]\n",
    "    #df_test = df_test[df_train.columns]\n",
    "    return (df_train, new_df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "#def del_zero_column(df_train, df_test):\n",
    "#    df_train = df_train.loc[:, (df_train != 0).any(axis=0)]\n",
    "#    df_test = df_test[df_train.columns]\n",
    "#    return (df_train, df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "\n",
    "def all_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_all_dummy = pd.get_dummies(train_combine)\n",
    "    #train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    (train_lda, lda) = lda_whole_function(train_all_dummy, np.array(label_num), n_cluster=n_cluster)\n",
    "    clf=BernoulliNB()\n",
    "    start = time.time()\n",
    "    \n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score , time.time()-start)\n",
    "\n",
    "def divide_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_column= train_combine.columns\n",
    "    for col in train_combine.columns:\n",
    "        train_dummy.append(pd.get_dummies(train_combine[col]))\n",
    "        #train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train_combine[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_combine.columns)):\n",
    "        #print train_dummy[i].shape\n",
    "        (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=3)\n",
    "        now_lda_col_name = []\n",
    "        for j in range(len(now_train_lda[0])):\n",
    "            train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score, time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "BinaryEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "HashingEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "HelmertEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "OneHotEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "OrdinalEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "SumEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "PolynomialEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(10):\n",
    "        print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_dna.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = category_encoders.BackwardDifferenceEncoder(cols=train.columns.tolist()).fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col_cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1b0145bf2590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/category_encoders/backward_difference.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordinal_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_difference_coding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_invariant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/category_encoders/backward_difference.pyc\u001b[0m in \u001b[0;36mbackward_difference_coding\u001b[1;34m(X_in, cols)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mbin_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C(%s, Diff)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdig\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36mdmatrix\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[1;32m--> 278\u001b[1;33m                                       NA_action, return_type)\n\u001b[0m\u001b[0;32m    279\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         raise PatsyError(\"encountered outcome variables for a model \"\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[1;32m--> 152\u001b[1;33m                                       NA_action)\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         return build_design_matrices(design_infos, data,\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/highlevel.pyc\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[1;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                       \u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                                       NA_action)\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/build.pyc\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[1;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                                                    NA_action)\n\u001b[0m\u001b[0;32m    697\u001b[0m     \u001b[1;31m# Now we need the factor infos, which encapsulate the knowledge of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;31m# how to turn any given factor into a chunk of data:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/build.pyc\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[1;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, memorize_state, data)\u001b[0m\n\u001b[0;32m    564\u001b[0m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[0;32m    565\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m                           data)\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[0m__getstate__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mno_pickling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36m_eval\u001b[1;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[0;32m    549\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eval_env\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m                                  \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                                  inner_namespace=inner_namespace)\n\u001b[0m\u001b[0;32m    552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmemorize_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_pass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/compat.pyc\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[1;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_wrap_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/patsy/eval.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"eval\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[1;32m--> 166\u001b[1;33m                                             + self._namespaces))\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'col_cap' is not defined"
     ]
    }
   ],
   "source": [
    "test.transform(train.iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "BinaryEncoder\n",
      "HashingEncoder\n",
      "HelmertEncoder\n",
      "OneHotEncoder\n",
      "OrdinalEncoder\n",
      "SumEncoder\n",
      "PolynomialEncoder\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor', 'gill_attachment', 'gill_spacing', 'gill_size',\\\n",
    "               'gill_color', 'stalk_shape', 'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring',\\\n",
    "               'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color', 'ring_number', 'ring_type',\\\n",
    "               'spore_print_color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(10):\n",
    "        #print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_mush.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "BinaryEncoder\n",
      "HashingEncoder\n",
      "HelmertEncoder\n",
      "OneHotEncoder\n",
      "OrdinalEncoder\n",
      "SumEncoder\n",
      "PolynomialEncoder\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(10):\n",
    "        #print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_car.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "BinaryEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HashingEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HelmertEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OneHotEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OrdinalEncoder\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "SumEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "PolynomialEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_5.binetflow')\n",
    "\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(5):\n",
    "        print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        train_dummy = train_dummy.join(train_numeric)\n",
    "        (train_dummy, test_dummy) = normalization(train_dummy, train_dummy.iloc[:5])\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10, scoring='f1'))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_bot5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "BinaryEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HashingEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HelmertEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OneHotEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OrdinalEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "SumEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "PolynomialEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_6.binetflow')\n",
    "\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(5):\n",
    "        print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        train_dummy = train_dummy.join(train_numeric)\n",
    "        (train_dummy, test_dummy) = normalization(train_dummy, train_dummy.iloc[:5])\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10, scoring='f1'))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_bot6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BackwardDifferenceEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "BinaryEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HashingEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "HelmertEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OneHotEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "OrdinalEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "SumEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "PolynomialEncoder\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_7.binetflow')\n",
    "\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "output_result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "exp_num = 0\n",
    "for encoder_name in category_encoders.__all__:\n",
    "    print encoder_name\n",
    "    encoder = getattr(category_encoders, encoder_name)\n",
    "    total_time = []\n",
    "    total_score = []\n",
    "    for iteration in xrange(5):\n",
    "        print iteration\n",
    "        train_dummy = encoder().fit_transform(train)\n",
    "        train_dummy = train_dummy.join(train_numeric)\n",
    "        (train_dummy, test_dummy) = normalization(train_dummy, train_dummy.iloc[:5])\n",
    "        start = time.time()\n",
    "        score = np.average(cross_val_score(clf, train_dummy, label_num, cv=10, scoring='f1'))\n",
    "        total_time.append(time.time()-start)\n",
    "        total_score.append(score)\n",
    "    output_result.loc[encoder_name] = [np.average(total_score), np.average(total_time)]\n",
    "    output_result.to_csv('all_encoder_bot7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
