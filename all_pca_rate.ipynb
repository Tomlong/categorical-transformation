{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders\n",
    "from numpy.linalg.linalg import LinAlgError\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations as Cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dummy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalization(training_numpy, testing_numpy):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testing_x_scaled = min_max.transform(testing_numpy)\n",
    "    #training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    #testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (x_scaled, testing_x_scaled)\n",
    "\n",
    "def pca_function(rate, data):\n",
    "    \n",
    "    try:\n",
    "        pca = PCA()\n",
    "        pca.fit(data)\n",
    "    except LinAlgError:\n",
    "        pca = PCA(n_components=data.shape[1]/2)\n",
    "        \n",
    "    for thres_n in xrange(1,len(data)):\n",
    "        if sum(pca.explained_variance_ratio_[:thres_n])>rate:\n",
    "            pca_n = thres_n\n",
    "            break\n",
    "    \n",
    "    pca = PCA(n_components=pca_n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data, pca)\n",
    "\n",
    "def lda_whole_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "def lda_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        print np.unique(original_y)\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "'''\n",
    "def pca_function(n, data):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data.flatten(), pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    lda = LDA()\n",
    "    lda_data = lda.fit_transform(X, y)\n",
    "    return (lda_data.flatten(), lda)\n",
    "'''\n",
    "def replace_value(training_normal_numpy, key, loc):  \n",
    "    training_normal_numpy[loc][key] = 1\n",
    "    \n",
    "def del_zero_column(df_train, df_test):\n",
    "    new_df_test = pd.DataFrame(np.zeros([df_test.shape[0], df_train.shape[1]]),columns=df_train.columns)\n",
    "    new_df_test[list(set(df_train.columns) & set(df_test.columns))] = df_test[list(set(df_train.columns) & set(df_test.columns))]\n",
    "    #df_test = df_test[df_train.columns]\n",
    "    return (df_train, new_df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "#def del_zero_column(df_train, df_test):\n",
    "#    df_train = df_train.loc[:, (df_train != 0).any(axis=0)]\n",
    "#    df_test = df_test[df_train.columns]\n",
    "#    return (df_train, df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "\n",
    "def all_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_all_dummy = pd.get_dummies(train_combine)\n",
    "    #train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    (train_lda, lda) = lda_whole_function(train_all_dummy, np.array(label_num), n_cluster=n_cluster)\n",
    "    clf=BernoulliNB()\n",
    "    start = time.time()\n",
    "    \n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score , time.time()-start)\n",
    "\n",
    "def divide_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_column= train_combine.columns\n",
    "    for col in train_combine.columns:\n",
    "        train_dummy.append(pd.get_dummies(train_combine[col]))\n",
    "        #train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train_combine[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_combine.columns)):\n",
    "        #print train_dummy[i].shape\n",
    "        (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=3)\n",
    "        now_lda_col_name = []\n",
    "        for j in range(len(now_train_lda[0])):\n",
    "            train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score, time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.9\n",
      "score: 0.82269338135\n",
      "time: 25.9504734039\n",
      "rate: 0.8\n",
      "score: 0.8222696862\n",
      "time: 20.9400804043\n",
      "rate: 0.7\n",
      "score: 0.821126159347\n",
      "time: 13.3753690243\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "\n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index])        \n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(accuracy_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('dna_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.9\n",
      "score: 0.886588286359\n",
      "time: 5.06185278893\n",
      "rate: 0.8\n",
      "score: 0.881102370834\n",
      "time: 4.21201767921\n",
      "rate: 0.7\n",
      "score: 0.878601820895\n",
      "time: 3.08991117477\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "\n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index])        \n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(accuracy_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('mush_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 117)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "pca.fit(train_dummy.iloc[train_val_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.9\n",
      "score: 0.705478892234\n",
      "time: 0.290058040619\n",
      "rate: 0.8\n",
      "score: 0.702971440825\n",
      "time: 0.297905778885\n",
      "rate: 0.7\n",
      "score: 0.703325178414\n",
      "time: 0.291408014297\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "\n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index])        \n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(accuracy_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('car_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Botnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.9\n",
      "score: 0.673220916713\n",
      "time: 1.33805937767\n",
      "rate: 0.8\n",
      "score: 0.710314093438\n",
      "time: 1.20119633675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.7\n",
      "score: 0.0\n",
      "time: 1.29103636742\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "train = pd.read_csv('botnet_5.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train = train[column_str]\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "\n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index])        \n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(f1_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('bot5_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rate: 0.9\n",
      "score: 0.974290850288\n",
      "time: 3.74724397659\n",
      "rate: 0.8\n",
      "score: 0.974295998172\n",
      "time: 3.49696464539\n",
      "rate: 0.7\n",
      "score: 0.784202274417\n",
      "time: 3.26707596779\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "train = pd.read_csv('botnet_6.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train = train[column_str]\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "\n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index])        \n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(f1_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('bot6_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1740, 44)\n",
      "rate: 0.9\n",
      "score: 0.0\n",
      "time: 0.420140171051\n",
      "rate: 0.8\n",
      "score: 0.0\n",
      "time: 0.419669771194\n",
      "rate: 0.7\n",
      "score: 0.0\n",
      "time: 0.422531414032\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "train = pd.read_csv('botnet_7.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "train = train[column_str]\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "print train_dummy.shape\n",
    "for rate in [0.9,0.8,0.7]:\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    for iteration in range(5):\n",
    "        start = time.time()\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            (train_pca, pca) = pca_function(rate, train_dummy.iloc[train_val_index]) \n",
    "            #print train_pca.shape\n",
    "            clf.fit(train_pca, val_label[train_val_index])\n",
    "            test_pca = pca.transform(train_dummy.iloc[test_val_index])\n",
    "            now_score.append(f1_score(clf.predict(test_pca), val_label[test_val_index]))\n",
    "        exe_time = time.time() - start    \n",
    "        #(score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"rate:\", rate\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(rate)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "result.to_csv('bot7_pca_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1677,   63])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
