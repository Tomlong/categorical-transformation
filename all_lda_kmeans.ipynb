{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders\n",
    "from numpy.linalg.linalg import LinAlgError\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations as Cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalization(training_numpy, testing_numpy):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testing_x_scaled = min_max.transform(testing_numpy)\n",
    "    #training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    #testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (x_scaled, testing_x_scaled)\n",
    "\n",
    "def pca_function(rate, data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    for thres_n in xrange(1,len(data)):\n",
    "        if sum(pca.explained_variance_ratio_[:thres_n])>rate:\n",
    "            pca_n = thres_n\n",
    "            break\n",
    "    \n",
    "    pca = PCA(n_components=pca_n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data, pca)\n",
    "\n",
    "def lda_whole_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "def lda_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        print np.unique(original_y)\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "'''\n",
    "def pca_function(n, data):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data.flatten(), pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    lda = LDA()\n",
    "    lda_data = lda.fit_transform(X, y)\n",
    "    return (lda_data.flatten(), lda)\n",
    "'''\n",
    "def replace_value(training_normal_numpy, key, loc):  \n",
    "    training_normal_numpy[loc][key] = 1\n",
    "    \n",
    "def del_zero_column(df_train, df_test):\n",
    "    new_df_test = pd.DataFrame(np.zeros([df_test.shape[0], df_train.shape[1]]),columns=df_train.columns)\n",
    "    new_df_test[list(set(df_train.columns) & set(df_test.columns))] = df_test[list(set(df_train.columns) & set(df_test.columns))]\n",
    "    #df_test = df_test[df_train.columns]\n",
    "    return (df_train, new_df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "#def del_zero_column(df_train, df_test):\n",
    "#    df_train = df_train.loc[:, (df_train != 0).any(axis=0)]\n",
    "#    df_test = df_test[df_train.columns]\n",
    "#    return (df_train, df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "\n",
    "def all_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_all_dummy = pd.get_dummies(train_combine)\n",
    "    #train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    (train_lda, lda) = lda_whole_function(train_all_dummy, np.array(label_num), n_cluster=n_cluster)\n",
    "    clf=BernoulliNB()\n",
    "    start = time.time()\n",
    "    \n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score , time.time()-start)\n",
    "\n",
    "def divide_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_column= train_combine.columns\n",
    "    for col in train_combine.columns:\n",
    "        train_dummy.append(pd.get_dummies(train_combine[col]))\n",
    "        #train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train_combine[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_combine.columns)):\n",
    "        #print train_dummy[i].shape\n",
    "        (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=3)\n",
    "        now_lda_col_name = []\n",
    "        for j in range(len(now_train_lda[0])):\n",
    "            train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score, time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.883683197919\n",
      "time: 0.00990114212036\n",
      "cluster: 3\n",
      "score: 0.87433398374\n",
      "time: 0.00768890380859\n",
      "cluster: 4\n",
      "score: 0.871999435616\n",
      "time: 0.012050930659\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            (train_lda,lda) = lda_whole_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster)        \n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(accuracy_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('dna_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.956922269593\n",
      "time: 0.0204949855804\n",
      "cluster: 3\n",
      "score: 0.909261628677\n",
      "time: 0.0162106990814\n",
      "cluster: 4\n",
      "score: 0.904765986785\n",
      "time: 0.0151782989502\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            (train_lda,lda) = lda_whole_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster)        \n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(accuracy_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)\n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('mush_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.671025732691\n",
      "time: 0.0108588218689\n",
      "cluster: 3\n",
      "score: 0.666246265146\n",
      "time: 0.0127231836319\n",
      "cluster: 4\n",
      "score: 0.662913166814\n",
      "time: 0.0109859466553\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            (train_lda,lda) = lda_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster)        \n",
    "            #print train_lda.shape\n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(accuracy_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)       \n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('car_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Botnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.0875542984573\n",
      "time: 0.0373637199402\n",
      "cluster: 3\n",
      "score: 0.154456256111\n",
      "time: 0.0322838068008\n",
      "cluster: 4\n",
      "score: 0.123737895992\n",
      "time: 0.0312878449758\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_5.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:               \n",
    "            (train_lda,lda) = lda_whole_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster) \n",
    "            train_lda = np.append(train_lda, train_numeric.iloc[train_val_index].values, axis=1)\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            test_lda = np.append(test_lda, train_numeric.iloc[test_val_index].values, axis=1)        \n",
    "            (train_lda, test_lda) = normalization(train_lda, test_lda)\n",
    "                 \n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(f1_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)      \n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('bot5_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.566807834502\n",
      "time: 0.0312148571014\n",
      "cluster: 3\n",
      "score: 0.578790922071\n",
      "time: 0.0285457611084\n",
      "cluster: 4\n",
      "score: 0.576957980091\n",
      "time: 0.0302193959554\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_6.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:               \n",
    "            (train_lda,lda) = lda_whole_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster) \n",
    "            train_lda = np.append(train_lda, train_numeric.iloc[train_val_index].values, axis=1)\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            test_lda = np.append(test_lda, train_numeric.iloc[test_val_index].values, axis=1)        \n",
    "            (train_lda, test_lda) = normalization(train_lda, test_lda)\n",
    "                 \n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(f1_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)      \n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('bot6_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2\n",
      "score: 0.0364285714286\n",
      "time: 0.0350560188293\n",
      "cluster: 3\n",
      "score: 0.167946608947\n",
      "time: 0.036266040802\n",
      "cluster: 4\n",
      "score: 0.231040885041\n",
      "time: 0.0346938610077\n"
     ]
    }
   ],
   "source": [
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "column_numeric = ['Dur', 'sTos', 'dTos', 'TotPkts', 'TotBytes','SrcBytes']\n",
    "train = pd.read_csv('botnet_7.binetflow')\n",
    "\n",
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "\n",
    "train_index = np.array(np.where(np.array(label_num) == 0)[0].tolist() + np.where(np.array(label_num) == 1)[0].tolist())\n",
    "train = train.iloc[train_index]\n",
    "train = train.fillna(0)\n",
    "label_num = np.array(label_num)[train_index]\n",
    "train_numeric = train[column_numeric]\n",
    "train = train[column_str]\n",
    "\n",
    "result = pd.DataFrame(columns=['score', 'time'])\n",
    "total_score = []\n",
    "total_time = []\n",
    "#clf = SVC(kernel='linear', C=1.0)\n",
    "clf = BernoulliNB()\n",
    "\n",
    "skf = StratifiedKFold(np.array(label_num), n_folds=10)\n",
    "val_label = np.array(label_num)\n",
    "    \n",
    "\n",
    "\n",
    "train_dummy = category_encoders.OneHotEncoder().fit_transform(train)\n",
    "for cluster in range(2,5):\n",
    "    for iteration in range(5):\n",
    "        val_time = []\n",
    "        now_score = []\n",
    "        for train_val_index, test_val_index in skf:               \n",
    "            (train_lda,lda) = lda_whole_function(train_dummy.iloc[train_val_index], val_label[train_val_index], n_cluster=cluster) \n",
    "            train_lda = np.append(train_lda, train_numeric.iloc[train_val_index].values, axis=1)\n",
    "            test_lda = lda.transform(train_dummy.iloc[test_val_index])\n",
    "            test_lda = np.append(test_lda, train_numeric.iloc[test_val_index].values, axis=1)        \n",
    "            (train_lda, test_lda) = normalization(train_lda, test_lda)\n",
    "                 \n",
    "            clf.fit(train_lda, val_label[train_val_index])\n",
    "            start = time.time()\n",
    "            now_score.append(f1_score(clf.predict(test_lda), val_label[test_val_index]))\n",
    "            val_time.append(time.time() - start)         \n",
    "        exe_time = np.sum(val_time)      \n",
    "        total_score.append(np.average(now_score))\n",
    "        total_time.append(exe_time)\n",
    "    print \"cluster:\", cluster\n",
    "    print \"score:\", np.average(total_score)\n",
    "    print \"time:\", np.average(total_time)\n",
    "    result.loc[str(cluster)] = [np.average(total_score), np.average(total_time)]\n",
    "\n",
    "    result.to_csv('bot7_lda_kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
