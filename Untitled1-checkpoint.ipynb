{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.lda import LDA\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class entryExit(object):\n",
    "\n",
    "    def __init__(self, f): \n",
    "        print 'entry init enter' \n",
    "        self.f = f \n",
    "        print 'entry init exit'\n",
    "\n",
    "    def __call__(self, *args): \n",
    "        print \"Entering\", self.f.__name__ \n",
    "        print \"Exited\", self.f.__name__ \n",
    "        return self.f(*args) \n",
    "\n",
    "print 'decorator using'\n",
    "\n",
    "@entryExit \n",
    "def hello(a): \n",
    "    print 'inside hello' \n",
    "    return \"hello world \" + a\n",
    "\n",
    "print 'test start' \n",
    "print hello('friends')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization(training_numpy, testing_numpy, columns):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testting_x_scaled = min_max.transform(testing_numpy)\n",
    "    training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (training_norm.values, testing_norm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "column_unuse = ['StartTime', 'SrcAddr', 'DstAddr', 'Sport']\n",
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "output_fscore = pd.DataFrame(columns=['fre_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_fscore', 'Lda_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_proto', 'Numeric_state', 'Numeric_dir', 'Numeric_dport', 'Binary_proto', 'Binary_state', 'Binary_dir','Binary_dport'])\n",
    "\n",
    "for index_train_dataset in xrange(1,14): \n",
    "   \n",
    "    train = pd.read_csv('botnet_'+str(index_train_dataset)+'.binetflow')\n",
    "\n",
    "    # Fill 0 to missing value\n",
    "    train = train.fillna(0)    \n",
    "    train = train[train['State'] != 0]\n",
    "\n",
    "    # Drop unuse columns and row with missing value\n",
    "    missing_state = []\n",
    "    map(lambda word: missing_state.append(word) if word[-1]=='_' else None, np.unique(train['State']))\n",
    "\n",
    "    for column in column_unuse:\n",
    "        del train[column]\n",
    "\n",
    "    for state in missing_state:\n",
    "        train = train[train['State'] != state]\n",
    "    \n",
    "\n",
    "    # Transfer str label to int lable\n",
    "    label_num = []\n",
    "\n",
    "    map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "    train['Label'] = label_num\n",
    "\n",
    "\n",
    "    \n",
    "    # training & testing index\n",
    "    train_index = np.where(train['Label'] == 0)[0].tolist() + np.where(train['Label'] == 1)[0].tolist()\n",
    "    print 'Dataset:',index_train_dataset, 'training length:', len(train_index)\n",
    "    skf = StratifiedKFold(np.array(label_num)[train_index], n_folds=10)\n",
    "    val_label = np.array(label_num)[train_index]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    all_score_num = []\n",
    "    now_num_fscore = []\n",
    "    \n",
    "\n",
    "       \n",
    "    clf_numeric = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "\n",
    "    #clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "    #clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "    \n",
    "    train_copy = train.iloc[train_index]\n",
    "    \n",
    "    for exp_iteration in range(1):\n",
    "        all_num_fscore = []\n",
    "        for train_val_index, test_val_index in skf:\n",
    "\n",
    "            state_numeric_dic = {}\n",
    "            proto_numeric_dic = {}\n",
    "            dir_numeric_dic = {}\n",
    "            dport_numeric_dic = {}\n",
    "            \n",
    "\n",
    "            unique_state = Counter(train_copy.iloc[train_val_index]['State'])\n",
    "            unique_proto = Counter(train_copy.iloc[train_val_index]['Proto'])\n",
    "            unique_dir = Counter(train_copy.iloc[train_val_index]['Dir'])\n",
    "            unique_dport = Counter(train_copy.iloc[train_val_index]['Dport'])\n",
    "\n",
    "            for key in unique_state.keys():\n",
    "                state_numeric_dic[key] = float(unique_state[key]) / len(train_val_index)\n",
    "            for key in unique_proto.keys():\n",
    "                proto_numeric_dic[key] = float(unique_proto[key]) / len(train_val_index)\n",
    "            for key in unique_dir.keys():\n",
    "                dir_numeric_dic[key] = float(unique_dir[key]) / len(train_val_index)\n",
    "            for key in unique_dport.keys():\n",
    "                dport_numeric_dic[key] = float(unique_dport[key]) / len(train_val_index)\n",
    "\n",
    "            state_numeric_array = []\n",
    "            proto_numeric_array = []\n",
    "            dir_numeric_array = []\n",
    "            dport_numeric_array = []\n",
    "\n",
    "            \n",
    "\n",
    "            map(lambda value: state_numeric_array.append(state_numeric_dic[value]) if value in state_numeric_dic.keys() else state_numeric_array.append(0), train.iloc[train_index]['State'])\n",
    "            map(lambda value: proto_numeric_array.append(proto_numeric_dic[value]) if value in proto_numeric_dic.keys() else proto_numeric_array.append(0), train.iloc[train_index]['Proto'])\n",
    "            map(lambda value: dir_numeric_array.append(dir_numeric_dic[value]) if value in dir_numeric_dic.keys() else dir_numeric_array.append(0), train.iloc[train_index]['Dir'])\n",
    "            map(lambda value: dport_numeric_array.append(dport_numeric_dic[value]) if value in dport_numeric_dic.keys() else dport_numeric_array.append(0), train.iloc[train_index]['Dport'])\n",
    "\n",
    "            \n",
    "            #Start processing numeric feature\n",
    "            train_copy_numeric = pd.DataFrame()\n",
    "        \n",
    "\n",
    "            state_numeric_array = np.array(state_numeric_array)\n",
    "            proto_numeric_array = np.array(proto_numeric_array)\n",
    "            dir_numeric_array = np.array(dir_numeric_array)\n",
    "            dport_numeric_array = np.array(dport_numeric_array)\n",
    "\n",
    "            train_copy_numeric['state_numeric'] = state_numeric_array\n",
    "            train_copy_numeric['proto_numeric'] = proto_numeric_array\n",
    "            train_copy_numeric['dir_numeric'] = dir_numeric_array\n",
    "            train_copy_numeric['dport_numeric'] = dport_numeric_array\n",
    "\n",
    "            \n",
    "            clf_numeric.fit(train_copy_numeric.iloc[train_val_index], val_label[train_val_index])\n",
    "            numeric_fs = f1_score(clf_numeric.predict(train_copy_numeric.iloc[test_val_index]), val_label[test_val_index])\n",
    "            all_num_fscore.append(numeric_fs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        now_num_fscore.append(np.average(all_num_fscore))\n",
    "\n",
    "    \n",
    "    print np.max(now_num_fscore), np.min(now_num_fscore), np.average(now_num_fscore)\n",
    "    output_fscore.loc[str(index_train_dataset)] = [np.average(now_num_fscore)]\n",
    "    #print output_fscore.loc[str(index_train_dataset)]\n",
    "    #output_fscore.to_csv('4_string_numeric_lda_pca_norm(10times).csv')\n",
    "    #output_fscore.loc[str(index_train_dataset)] = [numeric_fscore, lda_fscore]\n",
    "    output_fscore.to_csv('fre_num.csv')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1 training length: 61531\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.067288247987 0.067288247987 0.067288247987\n",
      "Dataset: 2 training length: 16733\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.638292889961 0.638292889961 0.638292889961\n",
      "Dataset: 3 training length: 119454\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.991074496924 0.991074496924 0.991074496924\n",
      "Dataset: 4 training length: 26701\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.940873629339 0.940873629339 0.940873629339\n",
      "Dataset: 5 training length: 5430\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.767927608645 0.767927608645 0.767927608645\n",
      "Dataset: 6 training length: 7720\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.922889948018 0.922889948018 0.922889948018\n",
      "Dataset: 7 training length: 1736\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.466736411736 0.466736411736 0.466736411736\n",
      "Dataset: 8 training length: 74206\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.940962501333 0.940962501333 0.940962501333\n",
      "Dataset: 9 training length: 166826\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.219472261702 0.219472261702 0.219472261702\n",
      "Dataset: 10 training length: 120928\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "column_unuse = ['StartTime', 'SrcAddr', 'DstAddr', 'Sport']\n",
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "output_fscore = pd.DataFrame(columns=['fre_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_fscore', 'Lda_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_proto', 'Numeric_state', 'Numeric_dir', 'Numeric_dport', 'Binary_proto', 'Binary_state', 'Binary_dir','Binary_dport'])\n",
    "\n",
    "for index_train_dataset in xrange(1,14): \n",
    "   \n",
    "    train = pd.read_csv('botnet_'+str(index_train_dataset)+'.binetflow')\n",
    "\n",
    "    # Fill 0 to missing value\n",
    "    train = train.fillna(0)    \n",
    "    train = train[train['State'] != 0]\n",
    "\n",
    "    # Drop unuse columns and row with missing value\n",
    "    missing_state = []\n",
    "    map(lambda word: missing_state.append(word) if word[-1]=='_' else None, np.unique(train['State']))\n",
    "\n",
    "    for column in column_unuse:\n",
    "        del train[column]\n",
    "\n",
    "    for state in missing_state:\n",
    "        train = train[train['State'] != state]\n",
    "    \n",
    "\n",
    "    # Transfer str label to int lable\n",
    "    label_num = []\n",
    "\n",
    "    map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "    train['Label'] = label_num\n",
    "\n",
    "\n",
    "    \n",
    "    # training & testing index\n",
    "    train_index = np.where(train['Label'] == 0)[0].tolist() + np.where(train['Label'] == 1)[0].tolist()\n",
    "    print 'Dataset:',index_train_dataset, 'training length:', len(train_index)\n",
    "    skf = StratifiedKFold(np.array(label_num)[train_index], n_folds=10)\n",
    "    val_label = np.array(label_num)[train_index]\n",
    "    \n",
    "    train_binary_proto = pd.get_dummies(train.iloc[train_index]['Proto'])\n",
    "    train_binary_state = pd.get_dummies(train.iloc[train_index]['State'])\n",
    "    train_binary_dir = pd.get_dummies(train.iloc[train_index]['Dir'])\n",
    "    train_binary_dport= pd.get_dummies(train.iloc[train_index]['Dport'])\n",
    "    \n",
    "    train_binary_concat = pd.concat([train_binary_proto, train_binary_state, train_binary_dir,train_binary_dport], axis=1)\n",
    "    \n",
    "    now_binary_fscore = []\n",
    "    \n",
    "\n",
    "       \n",
    "    clf_numeric = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "\n",
    "    #clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "    #clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "    \n",
    "    train_copy = train_binary_concat.copy()\n",
    "    \n",
    "    for exp_iteration in range(5):\n",
    "        all_num_fscore = []\n",
    "        print exp_iteration\n",
    "        for train_val_index, test_val_index in skf:\n",
    "        \n",
    "            \n",
    "            clf_numeric.fit(train_copy.iloc[train_val_index], val_label[train_val_index])\n",
    "            numeric_fs = f1_score(clf_numeric.predict(train_copy.iloc[test_val_index]), val_label[test_val_index])\n",
    "            all_num_fscore.append(numeric_fs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        now_binary_fscore.append(np.average(all_num_fscore))\n",
    "\n",
    "    \n",
    "    print np.max(now_binary_fscore), np.min(now_binary_fscore), np.average(now_binary_fscore)\n",
    "    output_fscore.loc[str(index_train_dataset)] = [np.average(now_binary_fscore)]\n",
    "    #print output_fscore.loc[str(index_train_dataset)]\n",
    "    #output_fscore.to_csv('4_string_numeric_lda_pca_norm(10times).csv')\n",
    "    #output_fscore.loc[str(index_train_dataset)] = [numeric_fscore, lda_fscore]\n",
    "    output_fscore.to_csv('one_hot_coding.csv')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_binary_proto.head(), train_binary_dir.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "column_unuse = ['StartTime', 'SrcAddr', 'DstAddr', 'Sport']\n",
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "output_fscore = pd.DataFrame(columns=['tf_idf_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_fscore', 'Lda_fscore'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_proto', 'Numeric_state', 'Numeric_dir', 'Numeric_dport', 'Binary_proto', 'Binary_state', 'Binary_dir','Binary_dport'])\n",
    "\n",
    "for index_train_dataset in xrange(1,14): \n",
    "   \n",
    "    train = pd.read_csv('botnet_'+str(index_train_dataset)+'.binetflow')\n",
    "\n",
    "    # Fill 0 to missing value\n",
    "    train = train.fillna(0)    \n",
    "    train = train[train['State'] != 0]\n",
    "\n",
    "    # Drop unuse columns and row with missing value\n",
    "    missing_state = []\n",
    "    map(lambda word: missing_state.append(word) if word[-1]=='_' else None, np.unique(train['State']))\n",
    "\n",
    "    for column in column_unuse:\n",
    "        del train[column]\n",
    "\n",
    "    for state in missing_state:\n",
    "        train = train[train['State'] != state]\n",
    "\n",
    "\n",
    "    # Transfer str label to int lable\n",
    "    label_num = []\n",
    "\n",
    "    map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "    train['Label'] = label_num\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # training & testing index\n",
    "    train_index = np.where(train['Label'] == 0)[0].tolist() + np.where(train['Label'] == 1)[0].tolist()\n",
    "    print 'Dataset:',index_train_dataset, 'training length:', len(train_index)\n",
    "    skf = StratifiedKFold(np.array(label_num)[train_index], n_folds=10)\n",
    "    val_label = np.array(label_num)[train_index]\n",
    "    \n",
    "    all_score_tfidf = []\n",
    "    now_tfidf_fscore = []\n",
    "    #now_lda_fscore = []\n",
    "    #now_pca_fscore = []\n",
    "    tfidf = TfidfTransformer()\n",
    "    vec = CountVectorizer(min_df=1)\n",
    "\n",
    "       \n",
    "    clf_tfidf = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "\n",
    "    #clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "    #clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "\n",
    "    for train_val_index, test_val_index in skf:\n",
    "        train_string_combine = []\n",
    "        test_string_combine = []\n",
    "        map(lambda line: train_string_combine.append(str(line[0])+' '+str(line[1])+' '+str(line[2])+' '+str(line[3])),train.iloc[train_val_index][column_str].values)\n",
    "        map(lambda line: test_string_combine.append(str(line[0])+' '+str(line[1])+' '+str(line[2])+' '+str(line[3])),train.iloc[test_val_index][column_str].values)\n",
    "\n",
    "        train_count_array = vec.fit_transform(train_string_combine).toarray()\n",
    "        test_count_array = vec.transform(test_string_combine).toarray()\n",
    "        train_tfidf_array = tfidf.fit_transform(train_count_array).toarray()\n",
    "        test_tfidf_array = tfidf.transform(test_count_array).toarray()\n",
    "\n",
    "        min_max = preprocessing.MinMaxScaler()\n",
    "        train_tfidf_norm = min_max.fit_transform(train_tfidf_array)\n",
    "        test_tfidf_norm = min_max.transform(test_tfidf_array)\n",
    "\n",
    "        #(train_numpy_tfidf_norm, test_numpy_tfidf_norm) = normalization(train_tfidf_array, test_tfidf_array, train_copy_lda.columns)\n",
    "\n",
    "        clf_tfidf.fit(train_tfidf_norm, val_label[train_val_index])\n",
    "        all_score_tfidf.append(f1_score(clf_tfidf.predict(test_tfidf_norm), val_label[test_val_index]))\n",
    "\n",
    "\n",
    "\n",
    "    tfidf_fscore = np.average(all_score_tfidf)\n",
    "\n",
    "    output_fscore.loc[str(index_train_dataset)] = [tfidf_fscore]\n",
    "    #print output_fscore.loc[str(index_train_dataset)]\n",
    "    #output_fscore.to_csv('4_string_numeric_lda_pca_norm(10times).csv')\n",
    "    #output_fscore.loc[str(index_train_dataset)] = [numeric_fscore, lda_fscore]\n",
    "    output_fscore.to_csv('tfidf.csv')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test binary\n",
    "test_binary = pd.DataFrame(columns=train_binary.columns)\n",
    "for column in train_binary.columns:\n",
    "    print column\n",
    "    #(train.iloc[train_index[100:200]]['Proto'] == column).dtype('float')\n",
    "    test_binary[column] = (train.iloc[train_index[100:200]]['Proto'] == column).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "vec = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_combine = []\n",
    "map(lambda line: string_combine.append(str(line[0])+' '+str(line[1])+' '+str(line[2])+' '+str(line[3])),train.iloc[train_index][column_str].values)\n",
    "#map(lambda line: string_combine.append(line['Proto']+'_'+line['State']+'_'+line['Dir']+'_'+line['Dport']),train.iloc[train_index])\n",
    "#map(lambda line: string_combine.append(str(line['Proto'])+'_'+str(line['State'])+'_'+str(line['Dir'])+'_'+str(line['Dport'])),train.iloc[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_array = vec.fit_transform(string_combine).toarray()\n",
    "tfidf_array = tfidf.fit_transform(count_array).toarray()\n",
    "clf = knn(n_neighbors=1)\n",
    "score = np.average(cross_val_score(clf, tfidf_array, np.array(label_num)[train_index], cv=10, scoring='f1'))\n",
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "test = [train.iloc[0]['Proto']+' '+train.iloc[0]['State']+' '+train.iloc[0]['Dir']+' '+train.iloc[0]['Dport'],\\\n",
    "        train.iloc[1]['Proto']+' '+train.iloc[1]['State']+' '+train.iloc[1]['Dir']+' '+train.iloc[1]['Dport']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
