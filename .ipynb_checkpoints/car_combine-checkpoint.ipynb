{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders\n",
    "from numpy.linalg.linalg import LinAlgError\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression as lr\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.cluster import KMeans\n",
    "from itertools import combinations as Cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalization(training_numpy, testing_numpy):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testing_x_scaled = min_max.transform(testing_numpy)\n",
    "    #training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    #testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (x_scaled, testing_x_scaled)\n",
    "\n",
    "def pca_function(rate, data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    for thres_n in xrange(1,len(data)):\n",
    "        if sum(pca.explained_variance_ratio_[:thres_n])>rate:\n",
    "            pca_n = thres_n\n",
    "            break\n",
    "    \n",
    "    pca = PCA(n_components=pca_n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data, pca)\n",
    "\n",
    "def lda_whole_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "def lda_function(X, y, n_cluster=2):\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    \n",
    "    \n",
    "    for uni_label in np.unique(y):\n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    #km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "        \n",
    "    #print len(anomaly_label),np.unique(anomaly_label[0])\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_uni_len = len(np.unique(anomaly_label[i]))\n",
    "        for (j,now_label) in zip(range(n_cluster*(i+1), n_cluster*(i+1)-n_cluster, -1), range(n_cluster-1,-1,-1)):\n",
    "            #print \"i\",i,\"j:\",j, \"now_label:\", now_label\n",
    "            anomaly_label[i][np.where(anomaly_label[i] == now_label)[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    #print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    #print np.unique(y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        print np.unique(original_y)\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "'''\n",
    "def pca_function(n, data):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data.flatten(), pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    lda = LDA()\n",
    "    lda_data = lda.fit_transform(X, y)\n",
    "    return (lda_data.flatten(), lda)\n",
    "'''\n",
    "def replace_value(training_normal_numpy, key, loc):  \n",
    "    training_normal_numpy[loc][key] = 1\n",
    "    \n",
    "def del_zero_column(df_train, df_test):\n",
    "    new_df_test = pd.DataFrame(np.zeros([df_test.shape[0], df_train.shape[1]]),columns=df_train.columns)\n",
    "    new_df_test[list(set(df_train.columns) & set(df_test.columns))] = df_test[list(set(df_train.columns) & set(df_test.columns))]\n",
    "    #df_test = df_test[df_train.columns]\n",
    "    return (df_train, new_df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "#def del_zero_column(df_train, df_test):\n",
    "#    df_train = df_train.loc[:, (df_train != 0).any(axis=0)]\n",
    "#    df_test = df_test[df_train.columns]\n",
    "#    return (df_train, df_test)\n",
    "\n",
    "def combine_function(x):\n",
    "    new_x = ''\n",
    "    for i in range(len(x)):\n",
    "        new_x = new_x + str(x[i]) +'_'\n",
    "    return new_x[:-1]\n",
    "\n",
    "def all_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    (train_lda, lda) = lda_whole_function(train_all_dummy, np.array(label_num), n_cluster=n_cluster)\n",
    "    clf=BernoulliNB()\n",
    "    start = time.time()\n",
    "    \n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score , time.time()-start)\n",
    "\n",
    "def divide_lda(train_combine, label_num, n_cluster = 2):\n",
    "    train_column= train_combine.columns\n",
    "    for col in train_combine.columns:\n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train_combine[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_combine.columns)):\n",
    "        #print train_dummy[i].shape\n",
    "        (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=3)\n",
    "        now_lda_col_name = []\n",
    "        for j in range(len(now_train_lda[0])):\n",
    "            train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    score = np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "    return (score, time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "result = pd.DataFrame(columns=['combine_num','c_cluster','score', 'time'])\n",
    "\n",
    "\n",
    "\n",
    "#combine all\n",
    "#combine_list = []\n",
    "#map(lambda data: combine_list.append(f(data)),train[car_columns[:-1]].values)\n",
    "#train_combine = pd.DataFrame(combine_list, columns=['combine'])\n",
    "train_dummy = []\n",
    "exp_num = 0\n",
    "for combine_num in range(2,5):\n",
    "    \n",
    "    train_combine = pd.DataFrame(index=range(len(train)))\n",
    "    com_index = 0\n",
    "    for sub in Cb(train.columns,combine_num):\n",
    "        combine_list = []\n",
    "        map(lambda data: combine_list.append(combine_function(data)),train[list(sub)].values)\n",
    "        train_combine['combine_%d' % com_index] = combine_list\n",
    "        com_index = com_index + 1\n",
    "    #print \"combine_subset:\", com_index\n",
    "    for cluster in range(2,6):\n",
    "        total_score = []\n",
    "        total_time = []\n",
    "        for iteration in range(10):\n",
    "            #print iteration\n",
    "            (score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "            clf=BernoulliNB()\n",
    "            start = time.time()\n",
    "            total_score.append(score)\n",
    "            total_time.append(exe_time)\n",
    "        print \"combine_num\", combine_num\n",
    "        print \"n_cluster:\", cluster\n",
    "        print \"score:\", np.average(total_score)\n",
    "        print \"time:\", np.average(total_time)\n",
    "        result.loc[exp_num] = [combine_num, cluster, np.average(total_score), np.average(total_time)]\n",
    "        exp_num = exp_num + 1\n",
    "        result.to_csv('mush_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "len lda: 214\n",
      "0.952990309741\n",
      "1\n",
      "len lda: 212\n",
      "0.952994228237\n",
      "2\n",
      "len lda: 213\n",
      "0.950489348659\n",
      "3\n",
      "len lda: 214\n",
      "0.94892487087\n",
      "4\n",
      "len lda: 213\n",
      "0.951426830118\n",
      "5\n",
      "len lda: 213\n",
      "0.950800850494\n",
      "6\n",
      "len lda: 214\n",
      "0.951416984473\n",
      "7\n",
      "len lda: 213\n",
      "0.950800850494\n",
      "8\n",
      "len lda: 214\n",
      "0.951108433833\n",
      "9\n",
      "len lda: 214\n",
      "0.951108433833\n",
      "0.951206114075 0.308605003357\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "combine_num = 3\n",
    "#divide combine\n",
    "train_combine = pd.DataFrame(index=range(len(train)))\n",
    "for i in range((len(train.columns)-combine_num)+1):\n",
    "    combine_list = []\n",
    "    map(lambda data: combine_list.append(combine_function(data)),train[train.columns[i:i+combine_num]].values)\n",
    "    train_combine['combine_%d' % i] = combine_list\n",
    "\n",
    "clf=BernoulliNB()\n",
    "total_score = []\n",
    "total_time = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    print iteration\n",
    "    for col in train_column:\n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train[[col]]))\n",
    "        \n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_column)-1):\n",
    "        if train_dummy[i].shape[1] >= len(np.unique(label_num)):\n",
    "            (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=2)\n",
    "            now_lda_col_name = []\n",
    "            for j in range(len(now_train_lda[0])):\n",
    "                train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    print \"len lda:\", len(train_lda.columns)\n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "    print np.average(cross_val_score(clf, train_lda, label_num, cv=10))\n",
    "print np.average(total_score), np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951109014196 0.288065314293\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(60):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "clf=BernoulliNB()\n",
    "total_score = []\n",
    "total_time = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    for col in train_column:\n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train[[col]]))\n",
    "        \n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(train_column)-1):\n",
    "        if train_dummy[i].shape[1] >= len(np.unique(label_num)):\n",
    "            #now_train_lda = lda.fit_transform(train_dummy[i], np.array(label_num)) // 1 dim\n",
    "            (now_train_lda, lda) = lda_whole_function(train_dummy[i], np.array(label_num), n_cluster=2)\n",
    "            now_lda_col_name = []\n",
    "            for j in range(len(now_train_lda[0])):\n",
    "                train_lda[train_column[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "    #print np.average(cross_val_score(clf, train_lda, label_num, cv=10)), time.time()-start\n",
    "print np.average(total_score), np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957073431189 0.41985244751\n"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('splice.data.txt', header=None)\n",
    "train.columns=['label', 'name', 'dna']\n",
    "key_str = {}\n",
    "for (i,label_name) in zip(range(len(np.unique(train['label']))), np.unique(train['label'])):\n",
    "    key_str[label_name] = i\n",
    "    \n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "train_dummy = []\n",
    "    \n",
    "train['dna'] = train['dna'].map(lambda x: list(str(x).strip()))\n",
    "for idx in xrange(10):\n",
    "    train['dna_%d'% (idx,)] = train['dna'].map(lambda x: x[idx])\n",
    "\n",
    "train_column = train.columns[3:]\n",
    "train = train[train_column]\n",
    "\n",
    "clf=BernoulliNB()\n",
    "total_score = []\n",
    "total_time = []\n",
    "\n",
    "for iteration in range(10):    \n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=train_column.tolist()).fit_transform(train)\n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_all_dummy, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "    #print np.average(cross_val_score(clf, train_all_dummy, label_num, cv=10))\n",
    "print np.average(total_score), np.average(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine_set: 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['combine_num','c_cluster','score', 'time'])\n",
    "\n",
    "\n",
    "\n",
    "#combine all\n",
    "#combine_list = []\n",
    "#map(lambda data: combine_list.append(f(data)),train[car_columns[:-1]].values)\n",
    "#train_combine = pd.DataFrame(combine_list, columns=['combine'])\n",
    "train_dummy = []\n",
    "exp_num = 0\n",
    "for combine_num in range(3,5):\n",
    "    \n",
    "    train_combine = pd.DataFrame(index=range(len(train)))\n",
    "    com_index = 0\n",
    "    for sub in Cb(train.columns,combine_num):\n",
    "        combine_list = []\n",
    "        map(lambda data: combine_list.append(combine_function(data)),train[list(sub)].values)\n",
    "        train_combine['combine_%d' % com_index] = combine_list\n",
    "        com_index = com_index + 1\n",
    "    print \"combine_set:\", com_index\n",
    "    #print \"combine_subset:\", com_index\n",
    "    for cluster in range(2,6):\n",
    "        total_score = []\n",
    "        total_time = []\n",
    "        for iteration in range(10):\n",
    "            #print iteration\n",
    "            (score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "            clf=BernoulliNB()\n",
    "            start = time.time()\n",
    "            total_score.append(score)\n",
    "            total_time.append(exe_time)\n",
    "        print \"combine_num\", combine_num\n",
    "        print \"n_cluster:\", cluster\n",
    "        print \"score:\", np.average(total_score)\n",
    "        print \"time:\", np.average(total_time)\n",
    "        result.loc[exp_num] = [combine_num, cluster, np.average(total_score), np.average(total_time)]\n",
    "        exp_num = exp_num + 1\n",
    "        result.to_csv('mush_result(combine3-5).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine_subset: 231\n",
      "231\n",
      "0\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-5f2224e85624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtrain_all_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_combine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_combine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mnow_train_lda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_all_dummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cluster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mtotal_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtotal_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-64fcf051f1a0>\u001b[0m in \u001b[0;36mlda_function\u001b[1;34m(X, y, n_cluster)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mlda_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;31m#print lda_data.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, store_covariance, tol)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shrinkage not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_svd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lsqr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_solve_lsqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshrinkage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36m_solve_svd\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfac\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mXc\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# SVD of centered (within)scaled data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/scipy/linalg/decomp_svd.pyc\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SVD did not converge\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         raise ValueError('illegal value in %d-th argument of internal gesdd'\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "combine_num = 2\n",
    "com_index = 0\n",
    "train_combine = pd.DataFrame(index=range(len(train)))\n",
    "for sub in Cb(train.columns,combine_num):\n",
    "    combine_list = []\n",
    "    map(lambda data: combine_list.append(combine_function(data)),train[list(sub)].values)\n",
    "    train_combine['combine_%d' % com_index] = combine_list\n",
    "    com_index = com_index + 1\n",
    "print \"combine_subset:\", com_index\n",
    "print len(train_combine.columns)\n",
    "\n",
    "\n",
    "\n",
    "combine_list = []\n",
    "map(lambda data: combine_list.append(f(data)),train[mush_columns[1:]].values)\n",
    "train_combine = pd.DataFrame(np.unique(combine_list), columns=['combine'])\n",
    "clf=BernoulliNB()\n",
    "\n",
    "for iteration in range(10):\n",
    "    print iteration\n",
    "    if train_dummy[i].shape[1] >= len(np.unique(label_num)):\n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    (now_train_lda, lda) = lda_function(train_all_dummy, np.array(label_num), n_cluster=2)\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    \n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, now_train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "del train['label']\n",
    "\n",
    "combine_list = []\n",
    "map(lambda data: combine_list.append(f(data)),train[mush_columns[1:]].values)\n",
    "train_combine = pd.DataFrame(np.unique(combine_list), columns=['combine'])\n",
    "clf=BernoulliNB()\n",
    "\n",
    "for iteration in range(10):\n",
    "    print iteration\n",
    "    for col in train_combine.columns:      \n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    \n",
    "    for i in range(len(train_combine.columns)):\n",
    "        #lda = LDA()\n",
    "        #print train_dummy[i]\n",
    "        if train_dummy[i].shape[1] >= len(np.unique(label_num)):\n",
    "            (now_train_lda, lda) = lda_function(train_dummy[i], np.array(label_num), n_cluster=2)\n",
    "            now_lda_col_name = train_combine.columns[i]\n",
    "            for j in range(len(now_train_lda[0])):\n",
    "                train_lda[now_lda_col_name+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    \n",
    "    #train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "    #(now_train_lda, lda) = lda_function(train_all_dummy, np.array(label_num), n_cluster=2)\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    \n",
    "    start = time.time()\n",
    "    #total_score.append(np.average(cross_val_score(clf, now_train_lda, label_num, cv=10)))\n",
    "    total_score.append(np.average(cross_val_score(clf, train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.955093989016\n",
      "time: 0.208826236725\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "\n",
    "\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "train_dummy = []\n",
    "\n",
    "\n",
    "clf=BernoulliNB()\n",
    "total_score = []\n",
    "total_time = []\n",
    "for iteration in range(10):\n",
    "    for col in mush_columns[1:]:      \n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train[[col]]))\n",
    "        \n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(mush_columns)-1):\n",
    "        #lda = LDA()\n",
    "        #print train_dummy[i]\n",
    "        if train_dummy[i].shape[1] >= len(np.unique(label_num)):\n",
    "            (now_train_lda, lda) = lda_function(train_dummy[i], np.array(label_num), n_cluster=2)\n",
    "            now_lda_col_name = []\n",
    "            for j in range(len(now_train_lda[0])):\n",
    "                train_lda[mush_columns[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "        #now_train_lda = lda.fit_transform(train_dummy[i].values, label_num)\n",
    "        \n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.920995900838\n",
      "time: 0.352157258987\n"
     ]
    }
   ],
   "source": [
    "mush_columns = ['label', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\\\n",
    "               'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\\\n",
    "               'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type',\\\n",
    "               'spore-print-color', 'population', 'habitat']\n",
    "#print len(mush_columns)\n",
    "train = pd.read_csv('agaricus-lepiota.data',header=None)\n",
    "train.columns = mush_columns\n",
    "key_str = {'e':0, 'p':1}\n",
    "\n",
    "\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "\n",
    "del train['label']\n",
    "\n",
    "total_score = []\n",
    "total_time = []\n",
    "for iteration in range(50):   \n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=mush_columns[1:]).fit_transform(train)\n",
    "    #print train_all_dummy.columns\n",
    "    clf=BernoulliNB()\n",
    "    #clf=SVC(C=1, gamma=0.1, class_weight='balanced')   \n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_all_dummy, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine_num 2\n",
      "n_cluster: 2\n",
      "score: 0.690326927004\n",
      "time: 0.0594995260239\n",
      "combine_num 2\n",
      "n_cluster: 3\n",
      "score: 0.755832955453\n",
      "time: 0.0653515100479\n",
      "combine_num 2\n",
      "n_cluster: 4\n",
      "score: 0.727796107609\n",
      "time: 0.0650562763214\n",
      "combine_num 2\n",
      "n_cluster: 5\n",
      "score: 0.734435992154\n",
      "time: 0.0581235170364\n",
      "combine_num 3\n",
      "n_cluster: 2\n",
      "score: 0.928761067125\n",
      "time: 0.0517038822174\n",
      "combine_num 3\n",
      "n_cluster: 3\n",
      "score: 0.841341767651\n",
      "time: 0.0481243371964\n",
      "combine_num 3\n",
      "n_cluster: 4\n",
      "score: 0.842552924775\n",
      "time: 0.052840256691\n",
      "combine_num 3\n",
      "n_cluster: 5\n",
      "score: 0.787309237197\n",
      "time: 0.0592196702957\n",
      "combine_num 4\n",
      "n_cluster: 2\n",
      "score: 0.98881503866\n",
      "time: 0.045464015007\n",
      "combine_num 4\n",
      "n_cluster: 3\n",
      "score: 0.981114127579\n",
      "time: 0.0485345125198\n",
      "combine_num 4\n",
      "n_cluster: 4\n",
      "score: 0.981699563792\n",
      "time: 0.0527772903442\n",
      "combine_num 4\n",
      "n_cluster: 5\n",
      "score: 0.982491378287\n",
      "time: 0.0535494804382\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "del train['label']\n",
    "\n",
    "result = pd.DataFrame(columns=['combine_num','c_cluster','score', 'time'])\n",
    "\n",
    "\n",
    "\n",
    "#combine all\n",
    "#combine_list = []\n",
    "#map(lambda data: combine_list.append(f(data)),train[car_columns[:-1]].values)\n",
    "#train_combine = pd.DataFrame(combine_list, columns=['combine'])\n",
    "train_dummy = []\n",
    "exp_num = 0\n",
    "for combine_num in range(2,5):\n",
    "    \n",
    "    train_combine = pd.DataFrame(index=range(len(train)))\n",
    "    com_index = 0\n",
    "    for sub in Cb(train.columns,combine_num):\n",
    "        combine_list = []\n",
    "        map(lambda data: combine_list.append(combine_function(data)),train[list(sub)].values)\n",
    "        train_combine['combine_%d' % com_index] = combine_list\n",
    "        com_index = com_index + 1\n",
    "    #print \"combine_subset:\", com_index\n",
    "    for cluster in range(2,6):\n",
    "        total_score = []\n",
    "        total_time = []\n",
    "        for iteration in range(10):\n",
    "            #print iteration\n",
    "            (score, exe_time) = all_lda(train_combine, label_num, cluster)       \n",
    "            clf=BernoulliNB()\n",
    "            start = time.time()\n",
    "            total_score.append(score)\n",
    "            total_time.append(exe_time)\n",
    "        print \"combine_num\", combine_num\n",
    "        print \"n_cluster:\", cluster\n",
    "        print \"score:\", np.average(total_score)\n",
    "        print \"time:\", np.average(total_time)\n",
    "        result.loc[exp_num] = [combine_num, cluster, np.average(total_score), np.average(total_time)]\n",
    "        exp_num = exp_num + 1\n",
    "        result.to_csv('car_result_divide.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.700258698827\n",
      "time: 1.1960580349\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "\n",
    "combine_list = []\n",
    "map(lambda data: combine_list.append(f(data)),train[car_columns[:-1]].values)\n",
    "train_combine = pd.DataFrame(combine_list, columns=['combine'])\n",
    "\n",
    "for iteration in range(10):\n",
    "\n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=train_combine.columns.tolist()).fit_transform(train_combine)\n",
    "\n",
    "    total_score = []\n",
    "    total_time = []\n",
    "    clf=BernoulliNB()\n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_all_dummy, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.816821836221\n",
      "time: 0.0642507457733\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "train_dummy = []\n",
    "clf=BernoulliNB()\n",
    "total_score = []\n",
    "total_time = []\n",
    "for iteration in range(50):\n",
    "    for col in car_columns[:-1]:\n",
    "        train_dummy.append(category_encoders.OneHotEncoder(cols=[col]).fit_transform(train[[col]]))\n",
    "    train_lda = pd.DataFrame(index=range(len(label_num)))\n",
    "    for i in range(len(car_columns)-1):\n",
    "        #lda = LDA()\n",
    "        #print train_dummy[i]\n",
    "        (now_train_lda, lda) = lda_function(train_dummy[i], np.array(label_num), n_cluster=4)\n",
    "        #now_train_lda = lda.fit_transform(train_dummy[i].values, label_num)\n",
    "        now_lda_col_name = []\n",
    "        for j in range(len(now_train_lda[0])):\n",
    "            train_lda[car_columns[i]+'_'+str(j+1)] = np.array(now_train_lda)[:,j]\n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_lda, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.772276033145\n",
      "time: 0.0672246408463\n"
     ]
    }
   ],
   "source": [
    "car_columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'label']\n",
    "train = pd.read_csv('car.data',header=None)\n",
    "train.columns = car_columns\n",
    "key_str = {'unacc':0, 'acc':1, 'good':2, 'vgood':3}\n",
    "label_num = []\n",
    "map(lambda label: map(lambda key: label_num.append(key_str[key]) if key == label else None,key_str.keys()), train['label'])\n",
    "train['label'] = label_num\n",
    "del train['label']\n",
    "\n",
    "total_score = []\n",
    "total_time = []\n",
    "for iteration in range(50):   \n",
    "    train_all_dummy = category_encoders.OneHotEncoder(cols=car_columns[:-1]).fit_transform(train)\n",
    "    clf=BernoulliNB()\n",
    "    #clf=SVC(C=1, gamma=0.1, class_weight='balanced')   \n",
    "    start = time.time()\n",
    "    total_score.append(np.average(cross_val_score(clf, train_all_dummy, label_num, cv=10)))\n",
    "    total_time.append(time.time()-start)\n",
    "print \"score:\", np.average(total_score)\n",
    "print \"time:\", np.average(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
