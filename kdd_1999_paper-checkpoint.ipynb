{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import time\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from numpy.linalg.linalg import LinAlgError\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix as confu\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "def normalization(training_numpy, testing_numpy):\n",
    "    min_max = preprocessing.MinMaxScaler()\n",
    "    min_max.fit(training_numpy)\n",
    "    x_scaled = min_max.fit_transform(training_numpy)\n",
    "    testing_x_scaled = min_max.transform(testing_numpy)\n",
    "    #training_norm = pd.DataFrame(x_scaled, columns = columns)\n",
    "    #testing_norm = pd.DataFrame(testting_x_scaled, columns = columns)\n",
    "    return (x_scaled, testing_x_scaled)\n",
    "\n",
    "def pca_function(rate, data):\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    for thres_n in xrange(1,len(data)):\n",
    "        if sum(pca.explained_variance_ratio_[:thres_n])>rate:\n",
    "            pca_n = thres_n\n",
    "            break\n",
    "    \n",
    "    pca = PCA(n_components=pca_n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data, pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    n_cluster = 2\n",
    "    #lda = LDA()\n",
    "    #lda_data = lda.fit_transform(X, y)\n",
    "    original_y = y.copy() \n",
    "    #normal_index = np.where(y == 0)[0]\n",
    "    anomaly_index = np.where(y == 1)[0]\n",
    "    #print len(normal_index), len(anomaly_index)\n",
    "    \n",
    "    #km_normal = KMeans(n_clusters=2)\n",
    "    #km_normal.fit(X.iloc[normal_index], y[normal_index])\n",
    "    km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly.fit(X.iloc[anomaly_index], y[anomaly_index])\n",
    "    \n",
    "   \n",
    "    #normal_label = km_normal.labels_    \n",
    "    #normal_label[np.where(normal_label == 0)[0]] = 0\n",
    "    #normal_label[np.where(normal_label == 1)[0]] = 1\n",
    "    \n",
    "    anomaly_label = km_anomaly.labels_\n",
    "    #print 'before label:',np.unique(anomaly_label)\n",
    "    for i in range(len(np.unique(anomaly_label)), 0, -1):\n",
    "        #print i, np.unique(anomaly_label)\n",
    "        anomaly_label[np.where(anomaly_label == (i-1))[0]] = i\n",
    "    #print 'label:',np.unique(anomaly_label)\n",
    "\n",
    "    #y[normal_index] = normal_label\n",
    "    y[anomaly_index] = anomaly_label\n",
    "    \n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)\n",
    "\n",
    "\n",
    "'''\n",
    "def pca_function(n, data):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return (pca_data.flatten(), pca)\n",
    "\n",
    "def lda_function(X, y):\n",
    "    lda = LDA()\n",
    "    lda_data = lda.fit_transform(X, y)\n",
    "    return (lda_data.flatten(), lda)\n",
    "'''\n",
    "def replace_value(training_normal_numpy, key, loc):  \n",
    "    training_normal_numpy[loc][key] = 1\n",
    "    \n",
    "def del_zero_column(df_train, df_test):\n",
    "    df_train = df_train.loc[:, (df_train != 0).any(axis=0)]\n",
    "    df_test = df_test[df_train.columns]\n",
    "    return (df_train, df_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "column_unuse = ['StartTime', 'SrcAddr', 'DstAddr', 'Sport']\n",
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "#output_fscore = pd.DataFrame()\n",
    "output_fscore = pd.DataFrame(columns=['max', 'min','average'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_proto', 'Numeric_state', 'Numeric_dir', 'Numeric_dport', 'Binary_proto', 'Binary_state', 'Binary_dir','Binary_dport'])\n",
    "\n",
    "pca_rate = 0.9\n",
    "\n",
    "\n",
    "for index_train_dataset in xrange(1,14): \n",
    "   \n",
    "    train = pd.read_csv('botnet_'+str(index_train_dataset)+'.binetflow')\n",
    "\n",
    "    # Fill 0 to missing value\n",
    "    train = train.fillna(0)    \n",
    "    train = train[train['State'] != 0]\n",
    "\n",
    "    # Drop unuse columns and row with missing value\n",
    "    missing_state = []\n",
    "    map(lambda word: missing_state.append(word) if word[-1]=='_' else None, np.unique(train['State']))\n",
    "\n",
    "    for column in column_unuse:\n",
    "        del train[column]\n",
    "\n",
    "    for state in missing_state:\n",
    "        train = train[train['State'] != state]\n",
    "\n",
    "\n",
    "    # Transfer str label to int lable\n",
    "    label_num = []\n",
    "\n",
    "    map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "    train['Label'] = label_num\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # training & testing index\n",
    "    train_index = np.array(np.where(train['Label'] == 0)[0].tolist() + np.where(train['Label'] == 1)[0].tolist())\n",
    "    print 'Dataset:',index_train_dataset, 'training length:', len(train_index)\n",
    "    \n",
    "    now_lda_fscore = []\n",
    "    \n",
    "    all_score_lda = []\n",
    "    \n",
    "    skf = StratifiedKFold(np.array(label_num)[train_index], n_folds=10)\n",
    "    val_label = np.array(label_num)[train_index]\n",
    "\n",
    "    train_binary_proto = pd.get_dummies(train.iloc[train_index]['Proto'])\n",
    "    train_binary_state = pd.get_dummies(train.iloc[train_index]['State'])\n",
    "    train_binary_dir = pd.get_dummies(train.iloc[train_index]['Dir'])\n",
    "    train_binary_dport= pd.get_dummies(train.iloc[train_index]['Dport'])\n",
    "\n",
    "    train_copy_lda = pd.DataFrame()\n",
    "    train_copy_pca = pd.DataFrame()\n",
    "    #for column in column_str:\n",
    "    #    del train_copy_lda[column]\n",
    "    #del train_copy_lda['Label']\n",
    "    \n",
    "    val_label = np.array(label_num)[train_index]\n",
    "    clf_lda = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "    clf_pca = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "    #clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "    #clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "    for iteration in range(5):\n",
    "        #print iteration\n",
    "        #ten_fold = 0\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            #print ten_fold\n",
    "            #ten_fold = ten_fold+1\n",
    "            ''' \n",
    "            train_binary_proto = pd.get_dummies(train.iloc[train_index[train_val_index]]['Proto'])\n",
    "            train_binary_state = pd.get_dummies(train.iloc[train_index[train_val_index]]['State'])\n",
    "            train_binary_dir = pd.get_dummies(train.iloc[train_index[train_val_index]]['Dir'])\n",
    "            train_binary_dport= pd.get_dummies(train.iloc[train_index[train_val_index]]['Dport'])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "\n",
    "            (train_proto_lda, proto_lda) = lda_function(train_binary_proto, val_label[train_val_index])\n",
    "            (train_state_lda, state_lda) = lda_function(train_binary_state, val_label[train_val_index])\n",
    "            (train_dir_lda, dir_lda) = lda_function(train_binary_dir, val_label[train_val_index])\n",
    "            (train_dport_lda, dport_lda) = lda_function(train_binary_dport, val_label[train_val_index])\n",
    "            train_lda_np = np.append(train_proto_lda, train_state_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dir_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dport_lda, 1)\n",
    "            \n",
    "            '''\n",
    "            #print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "            \n",
    "            (now_train_binary_proto,now_test_binary_proto) = del_zero_column(train_binary_proto.iloc[train_val_index],train_binary_proto.iloc[test_val_index])\n",
    "            (now_train_binary_state,now_test_binary_state) = del_zero_column(train_binary_state.iloc[train_val_index], train_binary_state.iloc[test_val_index])\n",
    "            (now_train_binary_dir,now_test_binary_dir) = del_zero_column(train_binary_dir.iloc[train_val_index], train_binary_dir.iloc[test_val_index])\n",
    "            (now_train_binary_dport,now_test_binary_dport) = del_zero_column(train_binary_dport.iloc[train_val_index], train_binary_dport.iloc[test_val_index])\n",
    "            \n",
    "            now_train = pd.DataFrame()\n",
    "            now_train[now_train_binary_proto.columns] = now_train_binary_proto\n",
    "            now_train[now_train_binary_state.columns] = now_train_binary_state\n",
    "            now_train[now_train_binary_dir.columns] = now_train_binary_dir\n",
    "            now_train[now_train_binary_dport.columns] = now_train_binary_dport\n",
    "            (train_data_lda, train_lda) = lda_function(now_train, val_label[train_val_index])\n",
    "            \n",
    "            now_test = pd.DataFrame()\n",
    "            now_test[now_test_binary_proto.columns] = now_test_binary_proto\n",
    "            now_test[now_test_binary_state.columns] = now_test_binary_state\n",
    "            now_test[now_test_binary_dir.columns] = now_test_binary_dir\n",
    "            now_test[now_test_binary_dport.columns] = now_test_binary_dport\n",
    "            test_data_lda= train_lda.transform(now_test)\n",
    "            \n",
    "            \n",
    "\n",
    "            #train_numpy_pca = train_copy_pca.values\n",
    "            #(train_numpy_pca_norm, test_numpy_pca_norm) = normalization(train_copy_pca.iloc[train_val_index],train_copy_pca.iloc[test_val_index], train_copy_pca.columns)\n",
    "            (train_numpy_lda_norm, test_numpy_lda_norm) = normalization(train_data_lda,test_data_lda)\n",
    "\n",
    "            #----Here--- continue\n",
    "            clf_lda.fit(train_numpy_lda_norm, val_label[train_val_index])\n",
    "            all_score_lda.append(f1_score(clf_lda.predict(test_numpy_lda_norm), val_label[test_val_index]))\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        lda_fscore = np.average(all_score_lda)\n",
    "        now_lda_fscore.append(lda_fscore)\n",
    "    print 'max:',np.max(now_lda_fscore),  'min:', np.min(now_lda_fscore),'avg:',np.average(now_lda_fscore)\n",
    "    output_fscore.loc[str(index_train_dataset)] = [np.max(now_lda_fscore), np.min(now_lda_fscore), np.average(now_lda_fscore)]\n",
    "    #print 'numeric:', numeric_fscore, 'lda:', lda_fscore \n",
    "\n",
    "\n",
    "    \n",
    "    #print output_fscore.loc[str(index_train_dataset)]\n",
    "    output_fscore.to_csv('lda_tine_3class(anomoly)(combine).csv')\n",
    "    #output_fscore.loc[str(index_train_dataset)] = [numeric_fscore, lda_fscore]\n",
    "    #output_fscore.to_csv('4_string_numeric_vs_lda_no_norm.csv')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1 training length: 61531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:455: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.252628269044 min: 0.20620873829 avg: 0.238645649589\n",
      "Dataset: 2 training length: 16733\n",
      "max: 0.687943666994 min: 0.653185680388 avg: 0.673920231259\n",
      "Dataset: 3 training length: 119454\n",
      "max: 0.979154637667 min: 0.97010099641 avg: 0.975869985089\n",
      "Dataset: 4 training length: 26701\n",
      "max: 0.941292539205 min: 0.934250065055 avg: 0.937159734857\n",
      "Dataset: 5 training length: 5430\n",
      "max: 0.76025319859 min: 0.752414725118 avg: 0.754974389195\n",
      "Dataset: 6 training length: 7720\n",
      "max: 0.869067158027 min: 0.858662264995 avg: 0.864345340591\n",
      "Dataset: 7 training length: 1736\n",
      "max: 0.457721088435 min: 0.441054421769 avg: 0.446957860922\n",
      "Dataset: 8 training length: 74206\n",
      "max: 0.905374848004 min: 0.868381068708 avg: 0.882517074573\n",
      "Dataset: 9 training length: 166826\n",
      "max: 0.591056096309 min: 0.529435814767 avg: 0.569325407863\n",
      "Dataset: 10 training length: 120928\n",
      "max: 0.99571099453 min: 0.994922459358 avg: 0.99519196068\n",
      "Dataset: 11 training length: 10791\n",
      "max: 0.998600851769 min: 0.998550489932 avg: 0.998568197291\n",
      "Dataset: 12 training length: 9788\n",
      "max: 0.883473396405 min: 0.86047245894 avg: 0.870085093913\n",
      "Dataset: 13 training length: 64411\n",
      "max: 0.806605004981 min: 0.804908105738 avg: 0.806266054573\n"
     ]
    }
   ],
   "source": [
    "key_str = {'Normal':0, 'Botnet':1, 'Background':2}\n",
    "column_unuse = ['StartTime', 'SrcAddr', 'DstAddr', 'Sport']\n",
    "column_str = ['Proto', 'State','Dir','Dport']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "#output_fscore = pd.DataFrame()\n",
    "output_fscore = pd.DataFrame(columns=['max', 'min','average'])\n",
    "#output_fscore = pd.DataFrame(columns=['Numeric_proto', 'Numeric_state', 'Numeric_dir', 'Numeric_dport', 'Binary_proto', 'Binary_state', 'Binary_dir','Binary_dport'])\n",
    "\n",
    "pca_rate = 0.9\n",
    "\n",
    "\n",
    "for index_train_dataset in xrange(1,14): \n",
    "   \n",
    "    train = pd.read_csv('botnet_'+str(index_train_dataset)+'.binetflow')\n",
    "\n",
    "    # Fill 0 to missing value\n",
    "    train = train.fillna(0)    \n",
    "    train = train[train['State'] != 0]\n",
    "\n",
    "    # Drop unuse columns and row with missing value\n",
    "    missing_state = []\n",
    "    map(lambda word: missing_state.append(word) if word[-1]=='_' else None, np.unique(train['State']))\n",
    "\n",
    "    for column in column_unuse:\n",
    "        del train[column]\n",
    "\n",
    "    for state in missing_state:\n",
    "        train = train[train['State'] != state]\n",
    "\n",
    "\n",
    "    # Transfer str label to int lable\n",
    "    label_num = []\n",
    "\n",
    "    map(lambda label: map(lambda key: label_num.append(key_str[key]) if key in label else None,key_str.keys()), train['Label'])\n",
    "    train['Label'] = label_num\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # training & testing index\n",
    "    train_index = np.array(np.where(train['Label'] == 0)[0].tolist() + np.where(train['Label'] == 1)[0].tolist())\n",
    "    print 'Dataset:',index_train_dataset, 'training length:', len(train_index)\n",
    "    \n",
    "    now_lda_fscore = []\n",
    "    \n",
    "    all_score_lda = []\n",
    "    \n",
    "    skf = StratifiedKFold(np.array(label_num)[train_index], n_folds=10)\n",
    "    val_label = np.array(label_num)[train_index]\n",
    "\n",
    "    train_binary_proto = pd.get_dummies(train.iloc[train_index]['Proto'])\n",
    "    train_binary_state = pd.get_dummies(train.iloc[train_index]['State'])\n",
    "    train_binary_dir = pd.get_dummies(train.iloc[train_index]['Dir'])\n",
    "    train_binary_dport= pd.get_dummies(train.iloc[train_index]['Dport'])\n",
    "\n",
    "    train_copy_lda = pd.DataFrame()\n",
    "    train_copy_pca = pd.DataFrame()\n",
    "    #for column in column_str:\n",
    "    #    del train_copy_lda[column]\n",
    "    #del train_copy_lda['Label']\n",
    "    \n",
    "    val_label = np.array(label_num)[train_index]\n",
    "    clf_lda = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "    clf_pca = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "    #clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "    #clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "    for iteration in range(10):\n",
    "        #print iteration\n",
    "        #ten_fold = 0\n",
    "        for train_val_index, test_val_index in skf:    \n",
    "            #print ten_fold\n",
    "            #ten_fold = ten_fold+1\n",
    "            ''' \n",
    "            train_binary_proto = pd.get_dummies(train.iloc[train_index[train_val_index]]['Proto'])\n",
    "            train_binary_state = pd.get_dummies(train.iloc[train_index[train_val_index]]['State'])\n",
    "            train_binary_dir = pd.get_dummies(train.iloc[train_index[train_val_index]]['Dir'])\n",
    "            train_binary_dport= pd.get_dummies(train.iloc[train_index[train_val_index]]['Dport'])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "            test_binary_proto = np.zeros([len(test_val_index), len(train_binary_proto.columns)])\n",
    "\n",
    "            (train_proto_lda, proto_lda) = lda_function(train_binary_proto, val_label[train_val_index])\n",
    "            (train_state_lda, state_lda) = lda_function(train_binary_state, val_label[train_val_index])\n",
    "            (train_dir_lda, dir_lda) = lda_function(train_binary_dir, val_label[train_val_index])\n",
    "            (train_dport_lda, dport_lda) = lda_function(train_binary_dport, val_label[train_val_index])\n",
    "            train_lda_np = np.append(train_proto_lda, train_state_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dir_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dport_lda, 1)\n",
    "            \n",
    "            '''\n",
    "            #print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "            \n",
    "            (now_train_binary_proto,now_test_binary_proto) = del_zero_column(train_binary_proto.iloc[train_val_index],train_binary_proto.iloc[test_val_index])\n",
    "            (now_train_binary_state,now_test_binary_state) = del_zero_column(train_binary_state.iloc[train_val_index], train_binary_state.iloc[test_val_index])\n",
    "            (now_train_binary_dir,now_test_binary_dir) = del_zero_column(train_binary_dir.iloc[train_val_index], train_binary_dir.iloc[test_val_index])\n",
    "            (now_train_binary_dport,now_test_binary_dport) = del_zero_column(train_binary_dport.iloc[train_val_index], train_binary_dport.iloc[test_val_index])\n",
    "            (train_proto_lda, proto_lda) = lda_function(now_train_binary_proto, val_label[train_val_index])\n",
    "            (train_state_lda, state_lda) = lda_function(now_train_binary_state, val_label[train_val_index])\n",
    "            (train_dir_lda, dir_lda) = lda_function(now_train_binary_dir, val_label[train_val_index])\n",
    "            (train_dport_lda, dport_lda) = lda_function(now_train_binary_dport, val_label[train_val_index])\n",
    "            train_lda_np = np.append(train_proto_lda, train_state_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dir_lda, 1)\n",
    "            train_lda_np = np.append(train_lda_np, train_dport_lda, 1)\n",
    "\n",
    "\n",
    "            test_proto_lda = proto_lda.transform(now_test_binary_proto)\n",
    "            test_state_lda = state_lda.transform(now_test_binary_state)            \n",
    "            test_dir_lda = dir_lda.transform(now_test_binary_dir)\n",
    "            test_dport_lda = dport_lda.transform(now_test_binary_dport)\n",
    "            test_lda_np = np.append(test_proto_lda, test_state_lda, 1)\n",
    "            test_lda_np = np.append(test_lda_np, test_dir_lda, 1)\n",
    "            test_lda_np = np.append(test_lda_np, test_dport_lda, 1)\n",
    "            \n",
    "            \n",
    "\n",
    "            #train_numpy_pca = train_copy_pca.values\n",
    "            #(train_numpy_pca_norm, test_numpy_pca_norm) = normalization(train_copy_pca.iloc[train_val_index],train_copy_pca.iloc[test_val_index], train_copy_pca.columns)\n",
    "            (train_numpy_lda_norm, test_numpy_lda_norm) = normalization(train_lda_np,test_lda_np)\n",
    "\n",
    "            #----Here--- continue\n",
    "            clf_lda.fit(train_numpy_lda_norm, val_label[train_val_index])\n",
    "            all_score_lda.append(f1_score(clf_lda.predict(test_numpy_lda_norm), val_label[test_val_index]))\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        lda_fscore = np.average(all_score_lda)\n",
    "        now_lda_fscore.append(lda_fscore)\n",
    "    print 'max:',np.max(now_lda_fscore),  'min:', np.min(now_lda_fscore),'avg:',np.average(now_lda_fscore)\n",
    "    output_fscore.loc[str(index_train_dataset)] = [np.max(now_lda_fscore), np.min(now_lda_fscore), np.average(now_lda_fscore)]\n",
    "    #print 'numeric:', numeric_fscore, 'lda:', lda_fscore \n",
    "\n",
    "\n",
    "    \n",
    "    #print output_fscore.loc[str(index_train_dataset)]\n",
    "    output_fscore.to_csv('lda_tine_5class(anomoly).csv')\n",
    "    #output_fscore.loc[str(index_train_dataset)] = [numeric_fscore, lda_fscore]\n",
    "    #output_fscore.to_csv('4_string_numeric_vs_lda_no_norm.csv')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_list = open('training_attack_types.txt')\n",
    "attack_dic = {}\n",
    "\n",
    "for line in attack_list.readlines():    \n",
    "    detail = line.split(' ')\n",
    "    if len(detail) == 1:\n",
    "        break\n",
    "    attack_type = detail[1][:-1]\n",
    "    attack_name = detail[0]\n",
    "    \n",
    "    if attack_dic.has_key(attack_type):\n",
    "        attack_dic[attack_type].append(attack_name)\n",
    "    else:\n",
    "        attack_dic[attack_type] = []\n",
    "        attack_dic[attack_type].append(attack_name)\n",
    "\n",
    "test_attack_dic = {}\n",
    "test_attack_dic[\"apache2.\"] = 2 \n",
    "test_attack_dic[\"back.\"] = 2 \n",
    "test_attack_dic[\"buffer_overflow.\"] = 3 \n",
    "test_attack_dic[\"ftp_write.\"] = 4 \n",
    "test_attack_dic[\"guess_passwd.\"] = 4 \n",
    "#test_attack_dic[\"httptunnel.\"] = 4 \n",
    "test_attack_dic[\"httptunnel.\"] = 3 \n",
    "test_attack_dic[\"imap.\"] = 4 \n",
    "test_attack_dic[\"ipsweep.\"] = 1 \n",
    "test_attack_dic[\"land.\"] = 2 \n",
    "test_attack_dic[\"loadmodule.\"] = 3 \n",
    "test_attack_dic[\"mailbomb.\"] = 2 \n",
    "test_attack_dic[\"mscan.\"] = 1 \n",
    "test_attack_dic[\"multihop.\"] = 4 \n",
    "#test_attack_dic[\"multihop.\"] = 3 # note that this is a duplicate \n",
    "test_attack_dic[\"named.\"] = 4 \n",
    "test_attack_dic[\"neptune.\"] = 2 \n",
    "test_attack_dic[\"nmap.\"] = 1 \n",
    "test_attack_dic[\"perl.\"] = 3 \n",
    "test_attack_dic[\"phf.\"] = 4 \n",
    "test_attack_dic[\"pod.\"] = 2 \n",
    "test_attack_dic[\"portsweep.\"] = 1 \n",
    "test_attack_dic[\"processtable.\"] = 2 \n",
    "test_attack_dic[\"ps.\"] = 3 \n",
    "test_attack_dic[\"rootkit.\"] = 3 \n",
    "test_attack_dic[\"saint.\"] = 1 \n",
    "test_attack_dic[\"satan.\"] = 1 \n",
    "test_attack_dic[\"sendmail.\"] = 4 \n",
    "test_attack_dic[\"smurf.\"] = 2 \n",
    "test_attack_dic[\"snmpgetattack.\"] = 4 \n",
    "test_attack_dic[\"snmpguess.\"] = 4 \n",
    "test_attack_dic[\"sqlattack.\"] = 3 \n",
    "test_attack_dic[\"teardrop.\"] = 2 \n",
    "test_attack_dic[\"udpstorm.\"] = 2 \n",
    "test_attack_dic[\"warezmaster.\"] = 4 \n",
    "test_attack_dic[\"worm.\"] = 4 \n",
    "test_attack_dic[\"xlock.\"] = 4 \n",
    "test_attack_dic[\"xsnoop.\"] = 4 \n",
    "test_attack_dic[\"xterm.\"] = 3\n",
    "test_attack_dic['normal.'] = 0\n",
    "attack_dic_num = {}\n",
    "attack_dic_num[0] = ['normal']\n",
    "attack_dic_num[2] = attack_dic['dos']\n",
    "attack_dic_num[4] = attack_dic['r2l']\n",
    "attack_dic_num[3] = attack_dic['u2r']\n",
    "attack_dic_num[1] = attack_dic['probe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = pd.DataFrame(np.zeros(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(3,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def del_zero_column(df_train, df_test):\n",
    "    new_df_test = pd.DataFrame(np.zeros([df_test.shape[0], df_train.shape[1]]),columns=df_train.columns)\n",
    "    new_df_test[list(set(df_train.columns) & set(df_test.columns))] = df_test[list(set(df_train.columns) & set(df_test.columns))]\n",
    "    #df_test = df_test[df_train.columns]\n",
    "    return (df_train, new_df_test)\n",
    "def lda_function(X, y):\n",
    "    n_cluster = 2\n",
    "    \n",
    "    original_y = y.copy()\n",
    "    anomaly_index = []\n",
    "    print \"y:\", np.unique(original_y)\n",
    "    \n",
    "    for uni_label in np.unique(y)[1:]:\n",
    "        print uni_label       \n",
    "        anomaly_index.append(np.where(y == uni_label)[0])\n",
    "\n",
    "    km_anomaly = KMeans(n_clusters=n_cluster)\n",
    "    km_anomaly_model = []\n",
    "    anomaly_label = []\n",
    "    for i in range(len(anomaly_index)):\n",
    "        km_anomaly_model.append(km_anomaly.fit(X.iloc[anomaly_index[i]], y[anomaly_index[i]]))\n",
    "        anomaly_label.append(km_anomaly.labels_)\n",
    "       \n",
    "    for i in range(len(anomaly_index)):\n",
    "        for (j,now_label) in zip(range(2*(i+1), 2*(i+1)-n_cluster, -1), range(n_cluster-1,0,-1)):\n",
    "            print \"i\",i,\"j:\",j\n",
    "            anomaly_label[i][np.where(anomaly_label == (now_label))[0]] = j\n",
    "        y[anomaly_index[i]] = anomaly_label[i]\n",
    "    print np.unique(y)\n",
    "    #print len(np.where(normal_label == 0)[0]),len(np.where(normal_label == 1)[0]), len(np.where(anomaly_label == 2)[0]), len(np.where(anomaly_label == 3)[0])\n",
    "    #print np.unique(y)\n",
    "    #return (X,y)\n",
    "    \n",
    "    #print lda.n_components\n",
    "    try:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, y)\n",
    "    except ValueError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "    except LinAlgError:\n",
    "        lda = LDA()\n",
    "        lda_data = lda.fit_transform(X, original_y)\n",
    "        \n",
    "    #print lda_data.shape\n",
    "    return (lda_data, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "training length: 30000\n",
      "del column finish\n",
      "[0 1 2 3 4]\n",
      "y: [0 1 2 3 4]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "i 0 j: 2\n",
      "i 1 j: 4\n",
      "i 2 j: 6\n",
      "i 3 j: 8\n",
      "[0 1]\n",
      "y: [0 1]\n",
      "1\n",
      "i 0 j: 2\n",
      "[0]\n",
      "y: [0]\n",
      "[0]\n",
      "train lda finish\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (6442,4) (9,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-ad4f4ba5197d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mtest_proto_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproto_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnow_test_binary_proto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[0mtest_state_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnow_test_binary_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mtest_flag_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnow_test_binary_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"test lda finish\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m#test_dport_lda = dport_lda.transform(now_test_binary_dport)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxbar_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalings_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eigen'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalings_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (6442,4) (9,) "
     ]
    }
   ],
   "source": [
    "column_str = ['protocol_type', 'service','flag']\n",
    "score_list = {}\n",
    "fscore_list = {}\n",
    "\n",
    "output_fscore = pd.DataFrame(columns=['max', 'min','average'])\n",
    "\n",
    "\n",
    "train = pd.read_csv('kdd_10_per.csv', nrows=30000)\n",
    "test= pd.read_csv('corrected', header=None, nrows=1000)\n",
    "test.columns = train.columns\n",
    "\n",
    "category_label_binary = []\n",
    "map(lambda label: map(lambda cate: category_label_binary.append(cate) if label in attack_dic_num[cate] else None,attack_dic_num.keys()),train['label'])\n",
    "category_label_binary = np.array(category_label_binary)\n",
    "#category_label_binary[np.where((category_label_binary != 0))] = 1\n",
    "\n",
    "print 'done'\n",
    "test_category_label_binary = []\n",
    "map(lambda label: test_category_label_binary.append(test_attack_dic[label]),test['label'])\n",
    "test_category_label_binary = np.array(test_category_label_binary)\n",
    "#test_category_label_binary[np.where((test_category_label_binary != 0))] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print 'training length:', len(train)\n",
    "\n",
    "now_lda_fscore = []\n",
    "\n",
    "all_score_lda = []\n",
    "\n",
    "skf = StratifiedKFold(train['label'], n_folds=10)\n",
    "val_label = train['label']\n",
    "\n",
    "train_binary_proto = pd.get_dummies(train['protocol_type'])\n",
    "train_binary_state = pd.get_dummies(train['service'])\n",
    "train_binary_flag = pd.get_dummies(train['flag'])\n",
    "#train_binary_dport= pd.get_dummies(train.iloc[train_index]['Dport'])\n",
    "\n",
    "test_binary_proto = pd.get_dummies(test['protocol_type'])\n",
    "test_binary_state = pd.get_dummies(test['service'])\n",
    "test_binary_flag = pd.get_dummies(test['flag'])\n",
    "\n",
    "del train['label']\n",
    "del test['label']\n",
    "train_copy_lda = pd.DataFrame()\n",
    "train_copy_pca = pd.DataFrame()\n",
    "#for column in column_str:\n",
    "#    del train_copy_lda[column]\n",
    "#del train_copy_lda['Label']\n",
    "\n",
    "#val_label = np.array(label_num)[train_index]\n",
    "clf_lda = knn(n_neighbors=1, algorithm='kd_tree', n_jobs=-1)\n",
    "clf_pca = knn(n_neighbors=1, algorithm='kd_tree')\n",
    "#clf_numeric = SVC(C=10,kernel='linear', class_weight='balanced')\n",
    "#clf_lda = SVC(C=10,kernel='linear',class_weight='balanced')\n",
    "\n",
    "        \n",
    "        #print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "\n",
    "(now_train_binary_proto,now_test_binary_proto) = del_zero_column(train_binary_proto, test_binary_proto)\n",
    "(now_train_binary_state,now_test_binary_state) = del_zero_column(train_binary_state, test_binary_state)\n",
    "(now_train_binary_flag,now_test_binary_flag) = del_zero_column(train_binary_flag, test_binary_flag)\n",
    "print \"del column finish\"\n",
    "#(now_train_binary_dport,now_test_binary_dport) = del_zero_column(train_binary_dport.iloc[train_val_index], train_binary_dport.iloc[test_val_index])\n",
    "#print np.unique(category_label_binary)\n",
    "(train_proto_lda, proto_lda) = lda_function(now_train_binary_proto, category_label_binary)\n",
    "(train_state_lda, state_lda) = lda_function(now_train_binary_state, category_label_binary)\n",
    "(train_flag_lda, flag_lda) = lda_function(now_train_binary_flag, category_label_binary)\n",
    "print \"train lda finish\"\n",
    "#(train_dport_lda, dport_lda) = lda_function(now_train_binary_dport, val_label[train_val_index])\n",
    "train_lda_np = np.append(train_proto_lda, train_state_lda, 1)\n",
    "train_lda_np = np.append(train_lda_np, train_flag_lda, 1)\n",
    "#train_lda_np = np.append(train_lda_np, train_dport_lda, 1)\n",
    "\n",
    "\n",
    "test_proto_lda = proto_lda.transform(now_test_binary_proto)\n",
    "test_state_lda = state_lda.transform(now_test_binary_state)            \n",
    "test_flag_lda = flag_lda.transform(now_test_binary_dir)\n",
    "print \"test lda finish\"\n",
    "#test_dport_lda = dport_lda.transform(now_test_binary_dport)\n",
    "test_lda_np = np.append(test_proto_lda, test_state_lda, 1)\n",
    "test_lda_np = np.append(test_lda_np, test_flag_lda, 1)\n",
    "#test_lda_np = np.append(test_lda_np, test_dport_lda, 1)\n",
    "\n",
    "\n",
    "\n",
    "#train_numpy_pca = train_copy_pca.values\n",
    "#(train_numpy_pca_norm, test_numpy_pca_norm) = normalization(train_copy_pca.iloc[train_val_index],train_copy_pca.iloc[test_val_index], train_copy_pca.columns)\n",
    "(train_numpy_lda_norm, test_numpy_lda_norm) = normalization(train_lda_np,test_lda_np)\n",
    "\n",
    "#----Here--- continue\n",
    "clf_lda.fit(train_numpy_lda_norm, category_label_binary)\n",
    "\n",
    "f_score = f1_score(clf_lda.predict(test_numpy_lda_norm), test_category_label_binary)\n",
    "\n",
    "print f_score\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    normal\n",
       "1    normal\n",
       "2    normal\n",
       "3    normal\n",
       "4    normal\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_data = lda.fit_transform(test, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit_transform(train_binary_proto.iloc[train_val_index], val_label[train_val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_proto_lda, proto_lda) = lda_function(train_binary_proto.iloc[train_val_index], val_label[train_val_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dport_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
